<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <link href="/style/blog.css" rel="stylesheet" />
  <link href="/style/home_style.css" rel="stylesheet" />
  <link href="/style/style.css" rel="stylesheet" />
  <link href="/style/posting.css" rel="stylesheet" />
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
  <title>Convolutional Neural Networks (CNNs)</title>
</head>

<body>
  <nav></nav>
  <article>
    <h2>Understanding Convolutional Neural Networks (CNNs)</h2>
    <section>
      <h3>What are Convolutional Neural Networks (CNNs)?</h3>
      <p>
        A Convolutional Neural Network (CNN) is a type of <strong>artificial neural network</strong> primarily used for
        processing <strong>visual data</strong>, such as images. CNNs excel at recognizing patterns, edges, shapes, and
        objects in images. Common applications include <strong>image classification</strong>, <strong>object
          detection</strong>, and <strong>image segmentation</strong>.
      </p>

      <h3>Key Components of CNNs</h3>
      <ul>
        <li>
          Convolution: This is the core operation of CNNs. It involves a filter (or kernel) sliding over the input
          (e.g., an image) and performing mathematical operations to extract features.
        </li>
        <li>
          Pooling: Pooling reduces the size of the feature map, helping to retain the most important features while
          improving computational efficiency.
        </li>
        <li>
          Activation Function (ReLU): The ReLU function replaces negative values with zero to introduce non-linearity.
        </li>
        <li>
          Fully Connected Layers: These layers summarize key patterns and make the final predictions.
        </li>
      </ul>

      <h4 id="important">Step1: Applying Convolution on a 4x4 Image</h4>
      <p>
        Imagine we have a 4x4 image grid and a 2x2 filter. As the filter slides over the image, it generates a feature
        map:
      </p>
      <pre>
        Image:
        [ [2, 0, 3, 0],
        [1, 3, 5, 1],
        [2, 2, 3, 0],
        [0, 1, 1, 0] ]

        Filter:
        [ [2, 3],
        [1, 0] ]
      </pre>

      Slides the filter to the input grid, and moving by 1 step (stride) would have following feature map by step:
      <pre>
        [2, 0] x [2, 3]
        [1, 3] x [1, 0]
        = (2×2) + (0×3) + (1×1) + (3×0) = 4 + 0 + 1 + 0 = 5

        [0, 3] x [2, 3]
        [3, 5] x [1, 0]
        = (0×2) + (3×3) + (3×1) + (5×0) = 0 + 9 + 3 + 0 = 12

        [3, 0] x [2, 3]
        [5, 1] x [1, 0]
        = (3×2) + (0×3) + (5×1) + (1×0) = 6 + 0 + 5 + 0 = 11

        [1, 3] x [2, 3]
        [2, 2] x [1, 0]
        = (1×2) + (3×3) + (2×1) + (2×0) = 2 + 9 + 2 + 0 = 13

        [3, 5] x [2, 3]
        [2, 3] x [1, 0]
        = (3×2) + (5×3) + (2×1) + (3×0) = 6 + 15 + 2 + 0 = 23

        [5, 1] x [2, 3]
        [3, 0] x [1, 0]
        = (5×2) + (1×3) + (3×1) + (0×0) = 10 + 3 + 3 + 0 = 16

        [2, 2] x [2, 3]
        [0, 1] x [1, 0]
        = (2×2) + (2×3) + (0×1) + (1×0) = 4 + 6 + 0 + 0 = 10

        [2, 3] x [2, 3]
        [1, 1] x [1, 0]
        = (2×2) + (3×3) + (1×1) + (1×0) = 4 + 9 + 1 + 0 = 14

        [3, 0] x [2, 3]
        [1, 0] x [1, 0]
        = (3×2) + (0×3) + (1×1) + (0×0) = 6 + 0 + 1 + 0 = 7

        Final feature map:
        [ [ 5, 12, 11 ],
        [ 13, 23, 16 ],
        [ 10, 14, 7 ] ]
      </pre>
      <h4 id="important">Step 2: Pooling</h4>
      <p>
        Extract the most important feature from the map and reduce the size of the feature map using map pooling. For
        example in an image of a dog, we can take the most important pixels of eyes, and ears to evaluate if it’s a dog
        or a cat.
      </p>
      <h4 id="important">Step 3-4: Activation & Fully Connected Layers</h4>
      <p>
        After convolution and pooling, the final feature maps are flattened into a vector 3x3x64 = 576
        identifying sets of features summarize key patterns in the image (edges, textures, shapes).
        The spatial dimensions (3×3) indicate the reduced size of the feature maps after multiple
        convolution and pooling operations. Applying 64 filters generates 64 distinct feature maps, capturing a rich
        variety of features. These feature maps are then flattened into a 1D vector of size 576 to serve as input for
        the fully connected layers. Using 64 filters strikes a balance between capturing detailed features and
        maintaining computational efficiency, making it a common choice in CNN architectures for tasks like image
        classification.
      </p>
      <p>
        In the process, an activation function like ReLU (Rectified Linear Unit) is applied, just like in other neural
        networks, to
        introduce non-linearity and make the model better at learning complex patterns.
        use softmax function convert the final output into a probability distribution.
        The softmax function converts the final output into a probability distribution. For example, if your model
        is trained to recognize 3 classes ("dog," "cat," or "rabbit"), softmax would output something like:
      <ul>
        <li>0.7 probability it’s a dog</li>
        <li>0.2 probability it’s a cat</li>
        <li>0.1 probability it’s a rabbit</li>
      </ul>
      </p>
      <h3>Text Classification with CNNs</h3>
      <p>
        While CNNs are widely used for image processing, they can also be applied to text classification tasks like
        <span id="important">sentiment analysis</span>. For instance, given an IMDB movie review dataset, we can
        classify reviews as positive or
        negative. Let's see how this works:
      </p>
      <h4>Step 1: Word Embeddings</h4>
      <p>
        Word embeddings are dense vector representations of words. These embeddings capture semantic relationships
        between words. For example:
      <pre>

        v("cat") = [0.12, -0.34, 0.56, ...]
        v("dog") = [0.13, -0.35, 0.58, ...]
      </pre>
      Since "cat" and "dog" are semantically similar, their vectors are close in the embedding space.
      </p>

      <h4>Step 2: CNN Model Architecture</h4>
      <pre>
        <code class="language-python">
          import torch
          import torch.nn as nn
          import torch.nn.functional as F

          class CNN(nn.Module):
          def __init__(self, vocab_size, embed_size, out_channels, filter_heights, stride, dropout, num_classes,
          pad_idx):
          super(CNN, self).__init__()
          # Creates a dense vector for each word
          self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=pad_idx)

          # Define a list of 1D convolutional layers, each with:
          # - `embed_size` as the number of input channels, corresponding to the embedding dimension of each word.
          # - `out_channels` as the number of filters (kernels) the convolutional layer will apply, controlling the
          number of features learned.
          # - `kernel_size` (or `filter_height`): Determines the size of the filter, effectively deciding how many words
          (e.g., 2-gram, 3-gram) the filter looks at.
          # - `stride`: Controls how far the filter moves at each step along the input sequence.
          # Multiple convolutional layers are created with different `filter_heights` to capture patterns of varying
          n-gram sizes (e.g., bigrams, trigrams).
          self.convo_layers = nn.ModuleList([
          nn.Conv1d(embed_size, out_channels, kernel_size=filter_height, stride=stride)
          for filter_height in filter_heights
          ])
          self.dropout = nn.Dropout(p=dropout)

          # Creates a fully connected dense layer combines features learned to make predictions
          self.fc_layer = nn.Linear(out_channels * len(filter_heights), num_classes)

          def forward(self, texts):
          embeddings = self.embedding(texts).permute(0, 2, 1)

          # Apply convolution, activation, and max pooling for each filter size
          feature_maps = [F.relu(conv(embedded)).max(dim=2)[0] for conv in self.convo_layers]
          concatenated_features = torch.cat(convo_outputs, dim=1)
          regularized_features = self.dropout(cnn_output)
          output = self.fc_layer(regularized_features)
          return output
        </code>
      </pre>

      <h4>Step 3: Fully Connected Layers</h4>
      <p>
        After extracting features through convolution and pooling, the output is passed to fully connected layers. These
        layers make the final prediction based on the extracted features. For binary classification (e.g., positive or
        negative sentiment), the final layer often uses a sigmoid activation function, which outputs a probability
        between 0 and 1.
      </p>
      <p>
        To summarize the CNN classifier:
        <br />
        N-Gram Features: Each convolutional layer extracts features corresponding to n-grams (sequences of filter_height
        words) in the text.<br />
        Parallel Convolution: Multiple filter heights allow the model to capture patterns of varying lengths
        simultaneously.<br />
        Max Pooling: Focuses on the most significant feature detected by each filter, reducing the dimensionality of the
        feature map.
      </p>
      <p>
        To conclude, CNNs are versatile tools that, although initially designed for image processing, have proven
        effective in
        text-based tasks. By leveraging embeddings, convolutional layers, and fully connected layers, CNNs can capture
        complex patterns and relationships, making them suitable for various NLP applications, including sentiment
        analysis and text classification.
      </p>
    </section>

  </article>
</body>
<script src="../../script.js" defer></script>

</html>